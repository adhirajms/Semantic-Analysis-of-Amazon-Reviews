{"cells": [{"cell_type": "markdown", "id": "5c0792da", "metadata": {}, "source": "### Importing Libraries"}, {"cell_type": "code", "execution_count": 1, "id": "9637b14e", "metadata": {}, "outputs": [], "source": "#import statements\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom pyspark.ml.feature import Bucketizer\n\nfrom pyspark.sql.functions import *\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nspark.conf.set(\"spark.sql.caseSensitive\", \"true\")"}, {"cell_type": "code", "execution_count": 2, "id": "6e32d9d0", "metadata": {}, "outputs": [], "source": "spark = SparkSession.builder.enableHiveSupport().appName('AmazonData').getOrCreate()\nsc = spark.sparkContext"}, {"cell_type": "code", "execution_count": 3, "id": "4efda457", "metadata": {}, "outputs": [{"data": {"text/plain": "[('spark.eventLog.enabled', 'true'),\n ('spark.yarn.historyServer.address', 'cluster-5430-m:18080'),\n ('spark.dynamicAllocation.minExecutors', '1'),\n ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n  'cluster-5430-m'),\n ('spark.sql.warehouse.dir', 'file:/spark-warehouse'),\n ('spark.executor.memory', '5739m'),\n ('spark.yarn.am.memory', '640m'),\n ('spark.driver.host', 'cluster-5430-m.us-central1-c.c.big-data-sm.internal'),\n ('spark.ui.proxyBase', '/proxy/application_1647055434597_0001'),\n ('spark.history.fs.logDirectory',\n  'gs://dataproc-temp-us-central1-599830097340-qhsl44mp/be55f996-47d0-404b-bcac-6b1d3bd744f2/spark-job-history'),\n ('spark.executor.instances', '2'),\n ('spark.app.startTime', '1647055574961'),\n ('spark.serializer.objectStreamReset', '100'),\n ('spark.yarn.unmanagedAM.enabled', 'true'),\n ('spark.sql.autoBroadcastJoinThreshold', '43m'),\n ('spark.submit.deployMode', 'client'),\n ('spark.extraListeners',\n  'com.google.cloud.spark.performance.DataprocMetricsListener'),\n ('spark.ui.filters',\n  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n ('spark.eventLog.dir',\n  'gs://dataproc-temp-us-central1-599830097340-qhsl44mp/be55f996-47d0-404b-bcac-6b1d3bd744f2/spark-job-history'),\n ('spark.sql.cbo.joinReorder.enabled', 'true'),\n ('spark.driver.maxResultSize', '1920m'),\n ('spark.app.id', 'application_1647055434597_0001'),\n ('spark.driver.port', '43895'),\n ('spark.shuffle.service.enabled', 'true'),\n ('spark.metrics.namespace',\n  'app_name:${spark.app.name}.app_id:${spark.app.id}'),\n ('spark.scheduler.mode', 'FAIR'),\n ('spark.sql.adaptive.enabled', 'true'),\n ('spark.yarn.jars', 'local:/usr/lib/spark/jars/*'),\n ('spark.scheduler.minRegisteredResourcesRatio', '0.0'),\n ('spark.executor.id', 'driver'),\n ('spark.hadoop.hive.execution.engine', 'mr'),\n ('spark.executor.cores', '2'),\n ('spark.app.name', 'PySparkShell'),\n ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'),\n ('spark.dynamicAllocation.maxExecutors', '10000'),\n ('spark.master', 'yarn'),\n ('spark.ui.port', '0'),\n ('spark.executorEnv.PYTHONPATH',\n  '/usr/lib/spark/python/lib/py4j-0.10.9-src.zip:/usr/lib/spark/python/:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9-src.zip'),\n ('spark.driver.appUIAddress',\n  'http://cluster-5430-m.us-central1-c.c.big-data-sm.internal:45181'),\n ('spark.sql.catalogImplementation', 'hive'),\n ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n  'http://cluster-5430-m:8088/proxy/application_1647055434597_0001'),\n ('spark.rdd.compress', 'True'),\n ('spark.rpc.message.maxSize', '512'),\n ('spark.executorEnv.OPENBLAS_NUM_THREADS', '1'),\n ('spark.submit.pyFiles', ''),\n ('spark.driver.memory', '3840m'),\n ('spark.dynamicAllocation.enabled', 'true'),\n ('spark.yarn.isPython', 'true'),\n ('spark.ui.showConsoleProgress', 'true'),\n ('spark.sql.cbo.enabled', 'true')]"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sparkContext.getConf().getAll()"}, {"cell_type": "markdown", "id": "3a9a7723", "metadata": {}, "source": "### Importing Datasets"}, {"cell_type": "code", "execution_count": 9, "id": "aba1f229", "metadata": {}, "outputs": [], "source": "path = 'gs://smbigdata1/main_final.csv'\ndf6 = spark.read.csv(path)"}, {"cell_type": "code", "execution_count": 10, "id": "976fd3db", "metadata": {}, "outputs": [], "source": "df6 = df6.withColumnRenamed(\"_c0\", \"uniqueID\") \\\n.withColumnRenamed(\"_c1\", \"productID\") \\\n.withColumnRenamed(\"_c2\", \"overall\") \\\n.withColumnRenamed(\"_c3\", \"reviewText\") \\\n.withColumnRenamed(\"_c4\", \"reviewTime\") \\\n.withColumnRenamed(\"_c5\", \"reviewerID\") \\\n.withColumnRenamed(\"_c6\", \"summary\") \\\n.withColumnRenamed(\"_c7\", \"unixReviewTime\") \\\n.withColumnRenamed(\"_c8\", \"verified\") \\\n.withColumnRenamed(\"_c9\", \"Category\") \\\n.withColumnRenamed(\"_c10\", \"brand\") \\\n.withColumnRenamed(\"_c11\", \"date\") \\\n.withColumnRenamed(\"_c12\", \"price\") \\\n.withColumnRenamed(\"_c13\", \"rank\") \\\n.withColumnRenamed(\"_c14\", \"title\") \\\n.withColumnRenamed(\"_c15\", \"timestamp\") \\\n.withColumnRenamed(\"_c16\", \"year\") \\\n.withColumnRenamed(\"_c17\", \"month\") \\\n.withColumnRenamed(\"_c18\", \"count\")"}, {"cell_type": "code", "execution_count": 11, "id": "1abed72f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "209338758"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "df6.count()"}, {"cell_type": "code", "execution_count": 12, "id": "2f05eb65", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+----------+-------+--------------------+-----------+--------------+--------------------+--------------+--------+--------------------+-----+------+-----+--------------------+--------------------+-------------------+----+-----+-----+\n|            uniqueID| productID|overall|          reviewText| reviewTime|    reviewerID|             summary|unixReviewTime|verified|            Category|brand|  date|price|                rank|               title|          timestamp|year|month|count|\n+--------------------+----------+-------+--------------------+-----------+--------------+--------------------+--------------+--------+--------------------+-----+------+-----+--------------------+--------------------+-------------------+----+-----+-----+\n|B00001OGXKA3SKEKM...|B00001OGXK|    2.0|The wig was a wil...| 09 3, 2015|A3SKEKMOD182WG|which wasn't grea...|    1441238400|    true|Clothing, Shoes a...| null|5 star| null|823,903inClothing...|Disguise - Adult ...|2015-09-03 00:00:00|2015|    9|   37|\n|B00001OGXKA3BE6HC...|B00001OGXK|    4.0|Lots of fun and g...|10 25, 2015| A3BE6HCM22ID9|          Four Stars|    1445731200|    true|Clothing, Shoes a...| null|5 star| null|823,903inClothing...|Disguise - Adult ...|2015-10-25 00:00:00|2015|   10|   37|\n|B00001OGXKA20E4R0...|B00001OGXK|    3.0|The wig was actua...|11 24, 2014|A20E4R0Y63VX43|The wig was actua...|    1416787200|    true|Clothing, Shoes a...| null|5 star| null|823,903inClothing...|Disguise - Adult ...|2014-11-24 00:00:00|2014|   11|   37|\n|B00001OGXKA1ZQJ9B...|B00001OGXK|    5.0|My son dressed up...|03 28, 2017|A1ZQJ9B0NX852R|   Halloween Costume|    1490659200|    true|Clothing, Shoes a...| null|5 star| null|823,903inClothing...|Disguise - Adult ...|2017-03-28 00:00:00|2017|    3|   37|\n|B00001OGXKA4KUVVA...|B00001OGXK|    3.0|The pants are rid...|01 10, 2014| A4KUVVASX10TY|Austin Powers outfit|    1389312000|    true|Clothing, Shoes a...| null|5 star| null|823,903inClothing...|Disguise - Adult ...|2014-01-10 00:00:00|2014|    1|   37|\n+--------------------+----------+-------+--------------------+-----------+--------------+--------------------+--------------+--------+--------------------+-----+------+-----+--------------------+--------------------+-------------------+----+-----+-----+\nonly showing top 5 rows\n\n"}], "source": "df6.show(5)"}, {"cell_type": "markdown", "id": "6930e4c0", "metadata": {}, "source": "### Converting review text into Term Frequencies"}, {"cell_type": "code", "execution_count": 23, "id": "9ced4df5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 67:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+----------+-------+-----------+--------------+--------------------+--------------+--------+----+--------------------+--------------------+------+-----+--------------------+--------------------+----------+-----+--------------------+\n|            uniqueID| productID|overall| reviewTime|    reviewerID|             summary|unixReviewTime|verified|vote|            Category|               brand|  date|price|                rank|               title|productID1|count|               words|\n+--------------------+----------+-------+-----------+--------------+--------------------+--------------+--------+----+--------------------+--------------------+------+-----+--------------------+--------------------+----------+-----+--------------------+\n|1519588135A2KUM7J...|1519588135|    3.0|12 18, 2016|A2KUM7JB88Z2NC|          Not to bad|    1482019200|    true|null|Clothing, Shoes a...|Visit Amazon's Je...|  null|$7.99|   2,024,632inBooks(|One White Lie (Th...|1519588135|  247|[it, wasn't, a, p...|\n|1519588135ASR774I...|1519588135|    5.0|02 22, 2018| ASR774IIR8VN7|          Five Stars|    1519257600|    true|null|Clothing, Shoes a...|Visit Amazon's Je...|  null|$7.99|   2,024,632inBooks(|One White Lie (Th...|1519588135|  247|[great, series, t...|\n|5120053084A1L2N7Z...|5120053084|    5.0|07 13, 2016|A1L2N7ZZY5SJWO|Double duty for Moms|    1468368000|    true|null|Clothing, Shoes a...|                null|5 star| null|87,615inClothing,...|sofsy Soft-Touch ...|5120053084|  346|[lovely, color, a...|\n|5120053084A1VSV6T...|5120053084|    5.0|08 29, 2016|A1VSV6TJ3TPCGQ|                BIG!|    1472428800|    true|null|Clothing, Shoes a...|                null|5 star| null|87,615inClothing,...|sofsy Soft-Touch ...|5120053084|  346|[i, like, this, t...|\n|5120053084A2GD1EW...|5120053084|    5.0|11 22, 2017|A2GD1EWQDLNOQ0|         Comfortable|    1511308800|    true|null|Clothing, Shoes a...|                null|5 star| null|87,615inClothing,...|sofsy Soft-Touch ...|5120053084|  346|[wearing, it, rig...|\n+--------------------+----------+-------+-----------+--------------+--------------------+--------------+--------+----+--------------------+--------------------+------+-----+--------------------+--------------------+----------+-----+--------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "\r                                                                                \r"}], "source": "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n\n#tokenize words\ntokenizer = Tokenizer(inputCol=\"reviewText\", outputCol=\"words\")\ndf6 = tokenizer.transform(df6)\n\n#drop the redundant source column\ndf6= df6.drop(\"reviewText\")\ndf6.show(5)"}, {"cell_type": "code", "execution_count": 24, "id": "311d7fef", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 82:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+----------+-------+-----------+--------------+--------------------+--------------+--------+----+--------------------+--------------------+------+-----+--------------------+--------------------+----------+-----+--------------------+\n|            uniqueID| productID|overall| reviewTime|    reviewerID|             summary|unixReviewTime|verified|vote|            Category|               brand|  date|price|                rank|               title|productID1|count|            filtered|\n+--------------------+----------+-------+-----------+--------------+--------------------+--------------+--------+----+--------------------+--------------------+------+-----+--------------------+--------------------+----------+-----+--------------------+\n|1519588135A2KUM7J...|1519588135|    3.0|12 18, 2016|A2KUM7JB88Z2NC|          Not to bad|    1482019200|    true|null|Clothing, Shoes a...|Visit Amazon's Je...|  null|$7.99|   2,024,632inBooks(|One White Lie (Th...|1519588135|  247|[page, turner, ge...|\n|1519588135ASR774I...|1519588135|    5.0|02 22, 2018| ASR774IIR8VN7|          Five Stars|    1519257600|    true|null|Clothing, Shoes a...|Visit Amazon's Je...|  null|$7.99|   2,024,632inBooks(|One White Lie (Th...|1519588135|  247|[great, series, r...|\n|5120053084A1L2N7Z...|5120053084|    5.0|07 13, 2016|A1L2N7ZZY5SJWO|Double duty for Moms|    1468368000|    true|null|Clothing, Shoes a...|                null|5 star| null|87,615inClothing,...|sofsy Soft-Touch ...|5120053084|  346|[lovely, color, w...|\n|5120053084A1VSV6T...|5120053084|    5.0|08 29, 2016|A1VSV6TJ3TPCGQ|                BIG!|    1472428800|    true|null|Clothing, Shoes a...|                null|5 star| null|87,615inClothing,...|sofsy Soft-Touch ...|5120053084|  346|[like, top, lot,,...|\n|5120053084A2GD1EW...|5120053084|    5.0|11 22, 2017|A2GD1EWQDLNOQ0|         Comfortable|    1511308800|    true|null|Clothing, Shoes a...|                null|5 star| null|87,615inClothing,...|sofsy Soft-Touch ...|5120053084|  346|[wearing, right, ...|\n+--------------------+----------+-------+-----------+--------------+--------------------+--------------+--------+----+--------------------+--------------------+------+-----+--------------------+--------------------+----------+-----+--------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "\r                                                                                \r"}], "source": "from pyspark.ml.feature import StopWordsRemover\n\n#remove stop words\nremover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\ndf6 = remover.transform(df6)\n\n#drop the redundant source column\ndf6= df6.drop(\"words\")\ndf6.show(5)"}, {"cell_type": "code", "execution_count": null, "id": "9948fa93", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 102:=====================================================>(99 + 1) / 100]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|features                                                                                                                                                                                                                                                                                                                                                                                                                |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|(20,[0,4,8,10,12,13,17,18],[1.5299720940841588,0.9852067172858053,0.6428740527963102,1.6858797024462338,1.1711688253282133,0.9338514684840521,0.7954704429968562,1.0698565250762404])                                                                                                                                                                                                                                   |\n|(20,[0,2,4,5,8,10,11,12,15,17,19],[0.7649860470420794,0.9652062785946628,1.9704134345716107,0.8448700121466444,1.2857481055926203,0.5619599008154113,2.888014341313921,1.7567532379923199,1.0056949919287863,0.7954704429968562,1.0289692438653908])                                                                                                                                                                    |\n|(20,[0,2,6,7,8,10,18,19],[0.7649860470420794,0.9652062785946628,0.7495402636031672,1.0040154569913895,0.6428740527963102,0.5619599008154113,1.0698565250762404,1.0289692438653908])                                                                                                                                                                                                                                     |\n|(20,[2,3,4,7,8,10,11,12,13,15],[1.9304125571893256,1.8738096741872712,1.9704134345716107,1.0040154569913895,0.6428740527963102,0.5619599008154113,0.9626714471046404,0.5855844126641067,0.9338514684840521,3.017084975786359])                                                                                                                                                                                          |\n|(20,[0,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],[0.7649860470420794,5.791237671567977,1.8738096741872712,2.955620151857416,3.3794800485865775,2.2486207908095013,5.0200772849569475,1.2857481055926203,3.7048401212483157,2.247839603261645,2.888014341313921,7.027012951969279,1.8677029369681042,1.027374652315977,1.0056949919287863,6.0812812678170225,8.750174872965419,2.139713050152481,4.115876975461563])|\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "\r                                                                                \r"}], "source": "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=20)\nfeaturizedData = hashingTF.transform(df6)\n\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nnlpdf = idfModel.transform(featurizedData)\nnlpdf.select(\"features\").show(5, truncate=False)"}, {"cell_type": "code", "execution_count": null, "id": "0a254788", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.feature import StringIndexer\n\nnlpdf_binary = nlpdf\n\nnlpdf_binary = nlpdf_binary.withColumn(\n'ratings',\nF.when((F.col(\"overall\") <= 5.0) & (F.col(\"overall\") >= 3.0), \"1\") \\\n    .when((F.col(\"overall\") < 3.0) & (F.col(\"overall\") >= 0), \"0\"))\n\nindexer = StringIndexer(inputCol=\"ratings\", outputCol=\"label\")\nindexer_model = indexer.fit(nlpdf_binary)\nnlpdf_binary=indexer_model.transform(nlpdf_binary)"}, {"cell_type": "code", "execution_count": 33, "id": "e93c465d", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.feature import StringIndexer\n\nnlpdf = nlpdf.withColumn(\n'ratings',\nF.when((F.col(\"overall\") <= 5.0) & (F.col(\"overall\") > 4.0), \"4-5\") \\\n    .when((F.col(\"overall\") <= 4.0) & (F.col(\"overall\") > 3.0), \"3-4\") \\\n    .when((F.col(\"overall\") <= 3.0) & (F.col(\"overall\") > 2.0), \"2-3\") \\\n    .when((F.col(\"overall\") <= 2.0) & (F.col(\"overall\") > 1.0), \"1-2\") \\\n    .when((F.col(\"overall\") <= 1.0) & (F.col(\"overall\") > 0), \"0-1\") \\\n)\n\nindexer = StringIndexer(inputCol=\"ratings\", outputCol=\"label\")\nindexer_model = indexer.fit(nlpdf)\nnlpdf=indexer_model.transform(nlpdf)"}, {"cell_type": "code", "execution_count": null, "id": "85234721", "metadata": {}, "outputs": [], "source": "#Splitting the data into test and train data\ntrain, test = nlpdf_binary.randomSplit(weights=[0.8,0.2], seed=200)"}, {"cell_type": "markdown", "id": "dead0224", "metadata": {}, "source": "### Running Binary Logistic Regression and Multi-Class Random Forest"}, {"cell_type": "code", "execution_count": null, "id": "9bb19e59", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "22/03/12 04:32:40 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n22/03/12 04:32:40 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "0.8740153695753506\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 311:====================================================>(199 + 1) / 200]\r"}, {"name": "stdout", "output_type": "stream", "text": "0.816253676344756\nCPU times: user 3.18 s, sys: 901 ms, total: 4.09 s\nWall time: 1h 4min\n"}, {"name": "stderr", "output_type": "stream", "text": "\r                                                                                \r"}], "source": "%%time\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n# Set parameters for Logistic Regression\nlgr = LogisticRegression(maxIter=10, featuresCol = 'features', labelCol='label')\n\n# Fit the model to the data.\nlgrm = lgr.fit(train)\n\n# Given a dataset, predict each point's label, and show the results.\npredictions = lgrm.transform(test)\n\n#print evaluation metrics\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n\nprint(evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"}))\nprint(evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}))"}, {"cell_type": "code", "execution_count": null, "id": "da22cf38", "metadata": {"collapsed": true}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 294:====================================================>(199 + 1) / 200]\r"}, {"ename": "NameError", "evalue": "name 'evaluator' is not defined", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "File \u001b[0;32m<timed exec>:14\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n", "\u001b[0;31mNameError\u001b[0m: name 'evaluator' is not defined"]}], "source": "%%time\nfrom pyspark.ml.classification import RandomForestClassifier\n\n#Set parameters for the Random Forest.\nrfc = RandomForestClassifier(maxDepth=5, numTrees=15, impurity=\"gini\", labelCol=\"label\", predictionCol=\"prediction\")\n\n#Fit the model to the data.\nrfcm = rfc.fit(train)\n\n#Given a dataset, predict each point's label, and show the results.\npredictions = rfcm.transform(test)\n\n#print evaluation metrics\n\n# print(evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"}))\n# print(evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}))"}, {"cell_type": "code", "execution_count": 37, "id": "83ffba25", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "0.6064639638414144\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 325:====================================================>(199 + 1) / 200]\r"}, {"name": "stdout", "output_type": "stream", "text": "0.4578982756124226\n"}, {"name": "stderr", "output_type": "stream", "text": "\r                                                                                \r"}], "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n\nprint(evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"}))\nprint(evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"}))"}, {"cell_type": "code", "execution_count": null, "id": "73621dde", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 5}